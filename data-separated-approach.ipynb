{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import plotly.express as px\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import spearmanr\n",
    "from math import sqrt\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/'\n",
    "df_X_train = pd.read_csv(data_path+'X_train.csv')#.drop(['COUNTRY'], axis=1).fillna(0) # given by the benchmark\n",
    "df_X_test = pd.read_csv(data_path+'X_test.csv')\n",
    "df_y_train = pd.read_csv(data_path+'Y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# French and Deutch sepration\n",
    "df = pd.merge(df_X_train, df_y_train, on='ID')\n",
    "df_X_train_fr = df.loc[df['COUNTRY'] == 'FR', df.columns[:-1]].drop(columns=['COUNTRY']).fillna(0)\n",
    "df_y_train_fr = df.loc[df['COUNTRY'] == 'FR', ['ID', 'TARGET']]\n",
    "\n",
    "df_X_train_de = df.loc[df['COUNTRY'] == 'DE', df.columns[:-1]].drop(columns=['COUNTRY']).fillna(0)\n",
    "df_y_train_de = df.loc[df['COUNTRY'] == 'DE', ['ID', 'TARGET']]\n",
    "\n",
    "df_X_test_fr = df_X_test.loc[df_X_test['COUNTRY'] == 'FR', df_X_test.columns].drop(columns=['COUNTRY']).fillna(0)\n",
    "df_X_test_de = df_X_test.loc[df_X_test['COUNTRY'] == 'DE', df_X_test.columns].drop(columns=['COUNTRY']).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples for the french dataset: 851\n",
      "Number of samples for the german dataset: 643\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of samples for the french dataset: {df_X_train_fr.shape[0]}\")\n",
    "print(f\"Number of samples for the german dataset: {df_X_train_de.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Time series?</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 13,\n",
       " 17,\n",
       " 19,\n",
       " 24,\n",
       " 30,\n",
       " 32,\n",
       " 33,\n",
       " 35,\n",
       " 39,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 51,\n",
       " 54,\n",
       " 56,\n",
       " 63,\n",
       " 67,\n",
       " 69,\n",
       " 79,\n",
       " 80,\n",
       " 84,\n",
       " 85,\n",
       " 95,\n",
       " 100,\n",
       " 103,\n",
       " 111,\n",
       " 113,\n",
       " 115,\n",
       " 123,\n",
       " 128,\n",
       " 129,\n",
       " 132,\n",
       " 133,\n",
       " 136,\n",
       " 139,\n",
       " 142,\n",
       " 148,\n",
       " 153,\n",
       " 156,\n",
       " 160,\n",
       " 161,\n",
       " 164,\n",
       " 167,\n",
       " 171,\n",
       " 173,\n",
       " 174,\n",
       " 176,\n",
       " 177,\n",
       " 190,\n",
       " 193,\n",
       " 196,\n",
       " 200,\n",
       " 202,\n",
       " 208,\n",
       " 210,\n",
       " 211,\n",
       " 213,\n",
       " 216,\n",
       " 218,\n",
       " 226,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 241,\n",
       " 244,\n",
       " 245,\n",
       " 252,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 265,\n",
       " 271,\n",
       " 275,\n",
       " 288,\n",
       " 291,\n",
       " 295,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 306,\n",
       " 316,\n",
       " 317,\n",
       " 324,\n",
       " 326,\n",
       " 332,\n",
       " 334,\n",
       " 338,\n",
       " 339,\n",
       " 342,\n",
       " 350,\n",
       " 352,\n",
       " 358,\n",
       " 360,\n",
       " 369,\n",
       " 370,\n",
       " 372,\n",
       " 373,\n",
       " 375,\n",
       " 376,\n",
       " 382,\n",
       " 386,\n",
       " 388,\n",
       " 390,\n",
       " 392,\n",
       " 394,\n",
       " 395,\n",
       " 397,\n",
       " 402,\n",
       " 404,\n",
       " 405,\n",
       " 407,\n",
       " 410,\n",
       " 411,\n",
       " 415,\n",
       " 421,\n",
       " 428,\n",
       " 432,\n",
       " 433,\n",
       " 435,\n",
       " 438,\n",
       " 440,\n",
       " 441,\n",
       " 447,\n",
       " 448,\n",
       " 451,\n",
       " 452,\n",
       " 455,\n",
       " 458,\n",
       " 459,\n",
       " 464,\n",
       " 468,\n",
       " 472,\n",
       " 475,\n",
       " 477,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 488,\n",
       " 489,\n",
       " 499,\n",
       " 502,\n",
       " 507,\n",
       " 513,\n",
       " 518,\n",
       " 520,\n",
       " 524,\n",
       " 527,\n",
       " 528,\n",
       " 531,\n",
       " 532,\n",
       " 541,\n",
       " 546,\n",
       " 548,\n",
       " 551,\n",
       " 559,\n",
       " 563,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 576,\n",
       " 577,\n",
       " 588,\n",
       " 590,\n",
       " 597,\n",
       " 599,\n",
       " 603,\n",
       " 607,\n",
       " 611,\n",
       " 613,\n",
       " 614,\n",
       " 618,\n",
       " 621,\n",
       " 625,\n",
       " 628,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 636,\n",
       " 646,\n",
       " 651,\n",
       " 658,\n",
       " 661,\n",
       " 670,\n",
       " 676,\n",
       " 691,\n",
       " 692,\n",
       " 694,\n",
       " 695,\n",
       " 703,\n",
       " 705,\n",
       " 707,\n",
       " 708,\n",
       " 710,\n",
       " 716,\n",
       " 719,\n",
       " 723,\n",
       " 726,\n",
       " 728,\n",
       " 731,\n",
       " 743,\n",
       " 745,\n",
       " 748,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 755,\n",
       " 759,\n",
       " 760,\n",
       " 764,\n",
       " 768,\n",
       " 772,\n",
       " 774,\n",
       " 784,\n",
       " 786,\n",
       " 800,\n",
       " 801,\n",
       " 810,\n",
       " 811,\n",
       " 813,\n",
       " 814,\n",
       " 819,\n",
       " 822,\n",
       " 825,\n",
       " 827,\n",
       " 829,\n",
       " 830,\n",
       " 834,\n",
       " 837,\n",
       " 839,\n",
       " 842,\n",
       " 843,\n",
       " 846,\n",
       " 849,\n",
       " 854,\n",
       " 857,\n",
       " 858,\n",
       " 860,\n",
       " 865,\n",
       " 866,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 873,\n",
       " 879,\n",
       " 880,\n",
       " 883,\n",
       " 889,\n",
       " 891,\n",
       " 892,\n",
       " 898,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 925,\n",
       " 926,\n",
       " 928,\n",
       " 937,\n",
       " 943,\n",
       " 946,\n",
       " 949,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 962,\n",
       " 966,\n",
       " 967,\n",
       " 970,\n",
       " 971,\n",
       " 976,\n",
       " 979,\n",
       " 981,\n",
       " 984,\n",
       " 991,\n",
       " 993,\n",
       " 997,\n",
       " 999,\n",
       " 1004,\n",
       " 1006,\n",
       " 1009,\n",
       " 1010,\n",
       " 1013,\n",
       " 1015,\n",
       " 1023,\n",
       " 1025,\n",
       " 1028,\n",
       " 1029,\n",
       " 1031,\n",
       " 1034,\n",
       " 1035,\n",
       " 1038,\n",
       " 1039,\n",
       " 1041,\n",
       " 1043,\n",
       " 1047,\n",
       " 1048,\n",
       " 1049,\n",
       " 1050,\n",
       " 1053,\n",
       " 1057,\n",
       " 1067,\n",
       " 1068,\n",
       " 1072,\n",
       " 1073,\n",
       " 1074,\n",
       " 1078,\n",
       " 1080,\n",
       " 1081,\n",
       " 1082,\n",
       " 1084,\n",
       " 1085,\n",
       " 1087,\n",
       " 1091,\n",
       " 1094,\n",
       " 1096,\n",
       " 1102,\n",
       " 1106,\n",
       " 1121,\n",
       " 1123,\n",
       " 1124,\n",
       " 1125,\n",
       " 1128,\n",
       " 1131,\n",
       " 1139,\n",
       " 1142,\n",
       " 1143,\n",
       " 1149,\n",
       " 1151,\n",
       " 1152,\n",
       " 1154,\n",
       " 1157,\n",
       " 1159,\n",
       " 1160,\n",
       " 1161,\n",
       " 1163,\n",
       " 1164,\n",
       " 1167,\n",
       " 1170,\n",
       " 1174,\n",
       " 1180,\n",
       " 1185,\n",
       " 1189,\n",
       " 1191,\n",
       " 1192,\n",
       " 1194,\n",
       " 1195,\n",
       " 1197,\n",
       " 1203,\n",
       " 1206,\n",
       " 1210,\n",
       " 1211,\n",
       " 1214]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_day_id = df_X_train_fr['DAY_ID'].to_list()\n",
    "sorted_day_id.sort()\n",
    "missing_day_ids = [day_id for day_id in list(range(0, 1216)) if day_id not in sorted_day_id]\n",
    "print(1214 - len(df_X_train_fr)) # last_day_id less the number of samples -> number of missing days between id 1 and 1214\n",
    "missing_day_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 13,\n",
       " 15,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 24,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 32,\n",
       " 33,\n",
       " 35,\n",
       " 39,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 47,\n",
       " 48,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 54,\n",
       " 56,\n",
       " 58,\n",
       " 59,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 67,\n",
       " 69,\n",
       " 75,\n",
       " 76,\n",
       " 79,\n",
       " 80,\n",
       " 84,\n",
       " 85,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 95,\n",
       " 100,\n",
       " 103,\n",
       " 110,\n",
       " 111,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 119,\n",
       " 123,\n",
       " 125,\n",
       " 126,\n",
       " 128,\n",
       " 129,\n",
       " 132,\n",
       " 133,\n",
       " 136,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 145,\n",
       " 148,\n",
       " 151,\n",
       " 153,\n",
       " 156,\n",
       " 157,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 164,\n",
       " 167,\n",
       " 171,\n",
       " 173,\n",
       " 174,\n",
       " 176,\n",
       " 177,\n",
       " 182,\n",
       " 185,\n",
       " 186,\n",
       " 190,\n",
       " 193,\n",
       " 196,\n",
       " 197,\n",
       " 200,\n",
       " 202,\n",
       " 206,\n",
       " 208,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 215,\n",
       " 216,\n",
       " 218,\n",
       " 224,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 238,\n",
       " 241,\n",
       " 242,\n",
       " 244,\n",
       " 245,\n",
       " 248,\n",
       " 249,\n",
       " 252,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 264,\n",
       " 265,\n",
       " 271,\n",
       " 273,\n",
       " 275,\n",
       " 283,\n",
       " 288,\n",
       " 291,\n",
       " 295,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 303,\n",
       " 306,\n",
       " 308,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 316,\n",
       " 317,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 326,\n",
       " 332,\n",
       " 334,\n",
       " 335,\n",
       " 338,\n",
       " 339,\n",
       " 342,\n",
       " 343,\n",
       " 350,\n",
       " 352,\n",
       " 354,\n",
       " 358,\n",
       " 360,\n",
       " 362,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 372,\n",
       " 373,\n",
       " 375,\n",
       " 376,\n",
       " 379,\n",
       " 382,\n",
       " 383,\n",
       " 385,\n",
       " 386,\n",
       " 388,\n",
       " 390,\n",
       " 392,\n",
       " 394,\n",
       " 395,\n",
       " 397,\n",
       " 401,\n",
       " 402,\n",
       " 404,\n",
       " 405,\n",
       " 407,\n",
       " 408,\n",
       " 410,\n",
       " 411,\n",
       " 415,\n",
       " 418,\n",
       " 421,\n",
       " 428,\n",
       " 432,\n",
       " 433,\n",
       " 435,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 443,\n",
       " 447,\n",
       " 448,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 455,\n",
       " 458,\n",
       " 459,\n",
       " 462,\n",
       " 464,\n",
       " 465,\n",
       " 468,\n",
       " 471,\n",
       " 472,\n",
       " 475,\n",
       " 477,\n",
       " 478,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 491,\n",
       " 493,\n",
       " 496,\n",
       " 498,\n",
       " 499,\n",
       " 502,\n",
       " 504,\n",
       " 506,\n",
       " 507,\n",
       " 509,\n",
       " 511,\n",
       " 513,\n",
       " 518,\n",
       " 520,\n",
       " 524,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 531,\n",
       " 532,\n",
       " 534,\n",
       " 541,\n",
       " 546,\n",
       " 548,\n",
       " 551,\n",
       " 555,\n",
       " 559,\n",
       " 563,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 581,\n",
       " 583,\n",
       " 585,\n",
       " 588,\n",
       " 590,\n",
       " 594,\n",
       " 596,\n",
       " 597,\n",
       " 599,\n",
       " 603,\n",
       " 607,\n",
       " 611,\n",
       " 613,\n",
       " 614,\n",
       " 618,\n",
       " 619,\n",
       " 621,\n",
       " 625,\n",
       " 628,\n",
       " 630,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 636,\n",
       " 637,\n",
       " 639,\n",
       " 646,\n",
       " 651,\n",
       " 657,\n",
       " 658,\n",
       " 661,\n",
       " 669,\n",
       " 670,\n",
       " 676,\n",
       " 680,\n",
       " 683,\n",
       " 688,\n",
       " 691,\n",
       " 692,\n",
       " 694,\n",
       " 695,\n",
       " 698,\n",
       " 699,\n",
       " 703,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 710,\n",
       " 716,\n",
       " 717,\n",
       " 719,\n",
       " 721,\n",
       " 723,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 731,\n",
       " 734,\n",
       " 737,\n",
       " 738,\n",
       " 740,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 755,\n",
       " 756,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 764,\n",
       " 766,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 772,\n",
       " 774,\n",
       " 777,\n",
       " 784,\n",
       " 786,\n",
       " 787,\n",
       " 793,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 804,\n",
       " 805,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 818,\n",
       " 819,\n",
       " 822,\n",
       " 825,\n",
       " 827,\n",
       " 829,\n",
       " 830,\n",
       " 834,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 842,\n",
       " 843,\n",
       " 845,\n",
       " 846,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 853,\n",
       " 854,\n",
       " 857,\n",
       " 858,\n",
       " 860,\n",
       " 863,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 879,\n",
       " 880,\n",
       " 883,\n",
       " 885,\n",
       " 886,\n",
       " 889,\n",
       " 891,\n",
       " 892,\n",
       " 894,\n",
       " 898,\n",
       " 900,\n",
       " 902,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 934,\n",
       " 937,\n",
       " 940,\n",
       " 943,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 949,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 955,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 962,\n",
       " 963,\n",
       " 966,\n",
       " 967,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 976,\n",
       " 978,\n",
       " 979,\n",
       " 981,\n",
       " 984,\n",
       " 986,\n",
       " 987,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 995,\n",
       " 997,\n",
       " 999,\n",
       " 1004,\n",
       " 1006,\n",
       " 1009,\n",
       " 1010,\n",
       " 1013,\n",
       " 1014,\n",
       " 1015,\n",
       " 1020,\n",
       " 1023,\n",
       " 1025,\n",
       " 1026,\n",
       " 1028,\n",
       " 1029,\n",
       " 1031,\n",
       " 1033,\n",
       " 1034,\n",
       " 1035,\n",
       " 1038,\n",
       " 1039,\n",
       " 1041,\n",
       " 1043,\n",
       " 1047,\n",
       " 1048,\n",
       " 1049,\n",
       " 1050,\n",
       " 1051,\n",
       " 1053,\n",
       " 1056,\n",
       " 1057,\n",
       " 1061,\n",
       " 1062,\n",
       " 1064,\n",
       " 1066,\n",
       " 1067,\n",
       " 1068,\n",
       " 1069,\n",
       " 1072,\n",
       " 1073,\n",
       " 1074,\n",
       " 1078,\n",
       " 1080,\n",
       " 1081,\n",
       " 1082,\n",
       " 1084,\n",
       " 1085,\n",
       " 1087,\n",
       " 1090,\n",
       " 1091,\n",
       " 1093,\n",
       " 1094,\n",
       " 1096,\n",
       " 1099,\n",
       " 1102,\n",
       " 1103,\n",
       " 1104,\n",
       " 1105,\n",
       " 1106,\n",
       " 1108,\n",
       " 1109,\n",
       " 1113,\n",
       " 1114,\n",
       " 1115,\n",
       " 1121,\n",
       " 1122,\n",
       " 1123,\n",
       " 1124,\n",
       " 1125,\n",
       " 1126,\n",
       " 1128,\n",
       " 1131,\n",
       " 1136,\n",
       " 1139,\n",
       " 1141,\n",
       " 1142,\n",
       " 1143,\n",
       " 1149,\n",
       " 1150,\n",
       " 1151,\n",
       " 1152,\n",
       " 1153,\n",
       " 1154,\n",
       " 1155,\n",
       " 1157,\n",
       " 1158,\n",
       " 1159,\n",
       " 1160,\n",
       " 1161,\n",
       " 1162,\n",
       " 1163,\n",
       " 1164,\n",
       " 1166,\n",
       " 1167,\n",
       " 1170,\n",
       " 1171,\n",
       " 1172,\n",
       " 1173,\n",
       " 1174,\n",
       " 1176,\n",
       " 1177,\n",
       " 1180,\n",
       " 1185,\n",
       " 1186,\n",
       " 1189,\n",
       " 1191,\n",
       " 1192,\n",
       " 1194,\n",
       " 1195,\n",
       " 1196,\n",
       " 1197,\n",
       " 1203,\n",
       " 1204,\n",
       " 1205,\n",
       " 1206,\n",
       " 1209,\n",
       " 1210,\n",
       " 1211,\n",
       " 1214,\n",
       " 1215]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_day_id = df_X_train_de['DAY_ID'].to_list()\n",
    "sorted_day_id.sort()\n",
    "missing_day_ids = [day_id for day_id in list(range(0, 1216)) if day_id not in sorted_day_id]\n",
    "print(1215 - df_X_test_de.shape[0]) # last_day_id less the number of samples -> number of missing days between id 1 and 1215\n",
    "missing_day_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data have a lot of missing values from day to day, the data are continuous over time, time series don't seems to be a good option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_spearman_correlation(y_predict, df_y_train):\n",
    "    print(spearmanr(y_predict, df_y_train['TARGET']).correlation)\n",
    "\n",
    "def get_spearman_cor(y_predict, df_y_train):\n",
    "    return(spearmanr(y_predict, df_y_train['TARGET']).correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.278678665030059\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "df_X_train.drop(['COUNTRY'], axis=1).fillna(0)\n",
    "# df_y_train['TARGET']\n",
    "model.fit(df_X_train.drop(['COUNTRY'], axis=1).fillna(0), df_y_train['TARGET'])\n",
    "# model.predict(df_X_train_fr)\n",
    "show_spearman_correlation(model.predict(df_X_train.drop(['COUNTRY'], axis=1).fillna(0)), df_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "french only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22230679657142144\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(df_X_train_fr, df_y_train_fr['TARGET'])\n",
    "# model.predict(df_X_train_fr)\n",
    "show_spearman_correlation(model.predict(df_X_train_fr), df_y_train_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "de only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43857978910369905\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(df_X_train_de, df_y_train_de['TARGET'])\n",
    "# model.predict(df_X_train_fr)\n",
    "show_spearman_correlation(model.predict(df_X_train_de), df_y_train_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman with alpha=10 : 0.23177915829632\n"
     ]
    }
   ],
   "source": [
    "model = Ridge(alpha=10)\n",
    "model.fit(df_X_train_fr, df_y_train_fr['TARGET'])\n",
    "y_predict_fr = model.predict(df_X_train_fr)\n",
    "print(f\"spearman with alpha=10 : {get_spearman_cor(y_predict_fr, df_y_train_fr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-09\n",
      "0.5\n",
      "1\n",
      "10\n",
      "100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "10000\n",
      "100000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles-m/Projects/master/ML for networks/project/.venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.07539e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/home/charles-m/Projects/master/ML for networks/project/.venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.09632e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/home/charles-m/Projects/master/ML for networks/project/.venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.08802e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/home/charles-m/Projects/master/ML for networks/project/.venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.15354e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/home/charles-m/Projects/master/ML for networks/project/.venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.10864e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alphas</th>\n",
       "      <th>cross-validation errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1e-09</td>\n",
       "      <td>0.737425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.736308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.735943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.733971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>0.728906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.724144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.724342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100000.0</td>\n",
       "      <td>0.782527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     alphas  cross-validation errors\n",
       "0     1e-09                 0.737425\n",
       "1       0.5                 0.736308\n",
       "2         1                 0.735943\n",
       "3        10                 0.733971\n",
       "4       100                 0.728906\n",
       "5      1000                 0.724144\n",
       "6     10000                 0.724342\n",
       "7  100000.0                 0.782527"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "alphas = [1e-9, 0.5, 1, 10, 100, 1000, 10000, 1e5] #, 1e6]\n",
    "errors = []\n",
    "# alphas = [5, 10, 15, 20, 25, 30, 50, 100]\n",
    "for a in alphas:\n",
    "    print(a)\n",
    "    scores = cross_val_score(Ridge(alpha=a),\n",
    "                df_X_train_fr, df_y_train_fr, cv=k_fold,\n",
    "                scoring = 'neg_mean_squared_error')\n",
    "    errors.append(sqrt(mean(-scores) ) )\n",
    "\n",
    "# We convert x-values to string because otherwise they would\n",
    "# be to far from each other\n",
    "df_errors = pd.DataFrame({'alphas':[str(a) for a in alphas],\n",
    "                        'cross-validation errors':errors})\n",
    "ax = df_errors.plot.bar(x='alphas')\n",
    "df_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In numerical analysis, the condition number of a function measures how much the output value of the function can change for a small change in the input argument. This is used to measure how sensitive a function is to changes or errors in the output, and how much error in the output results from an error in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman with alpha=1000 : 0.23177915829632\n"
     ]
    }
   ],
   "source": [
    "model = Ridge(alpha=10)\n",
    "model.fit(df_X_train_fr, df_y_train_fr['TARGET'])\n",
    "y_predict_fr = model.predict(df_X_train_fr)\n",
    "print(f\"spearman with alpha=1000 : {get_spearman_cor(y_predict_fr, df_y_train_fr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearman with alpha=5 : 0.4414594599303667\n"
     ]
    }
   ],
   "source": [
    "model = Ridge(alpha=5) # 5 finded manually by trying several values\n",
    "model.fit(df_X_train_de, df_y_train_de['TARGET'])\n",
    "y_predict_de = model.predict(df_X_train_de)\n",
    "print(f\"spearman with alpha=5 : {get_spearman_cor(y_predict_de, df_y_train_de)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression with cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for polynomial degree 2\n",
      "0.5399035699245784\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for polynomial degree 3\n",
      "0.9999997274028819\n",
      "\n",
      "result for polynomial degree 4\n",
      "0.9999997274028819\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "for i in range(2,5):\n",
    "    poly_var = PolynomialFeatures(degree=i).fit_transform(df_X_train_fr)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(poly_var, df_y_train_fr['TARGET'])\n",
    "    y_predict_fr = lr.predict(poly_var)\n",
    "    print(f'result for polynomial degree {i}')\n",
    "    show_spearman_correlation(y_predict_fr, df_y_train_fr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for polynomial degree 2\n",
      "0.7735254179158632\n",
      "\n",
      "result for polynomial degree 3\n",
      "0.9999999548612647\n",
      "\n",
      "result for polynomial degree 4\n",
      "0.9999999548612647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "for i in range(2,5):\n",
    "    poly_var = PolynomialFeatures(degree=i).fit_transform(df_X_train_de)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(poly_var, df_y_train_de['TARGET'])\n",
    "    y_predict_de = lr.predict(poly_var)\n",
    "    print(f'result for polynomial degree {i}')\n",
    "    show_spearman_correlation(y_predict_de, df_y_train_de)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 630)\n",
      "spearman correlation for the train dataset : 0.6191303014726739\n",
      "(151, 630)\n",
      "spearman correlation for the test dataset (unseen data) : -0.08157894736842104\n"
     ]
    }
   ],
   "source": [
    "x_train_fr = df_X_train_fr[:700]\n",
    "y_train_fr = df_y_train_fr[:700]\n",
    "x_test_fr = df_X_train_fr[700:]\n",
    "y_test_fr = df_y_train_fr[700:]\n",
    "\n",
    "\n",
    "poly_var = PolynomialFeatures(degree=2).fit_transform(x_train_fr)\n",
    "print(poly_var.shape)\n",
    "lr = LinearRegression()\n",
    "lr.fit(poly_var, y_train_fr['TARGET'])\n",
    "y_predict_fr = lr.predict(poly_var)\n",
    "\n",
    "print(f\"spearman correlation for the train dataset : {get_spearman_cor(y_predict_fr, y_train_fr)}\")\n",
    "poly_var_test = PolynomialFeatures(degree=2).fit_transform(x_test_fr)\n",
    "print(poly_var_test.shape)\n",
    "y_test_predict_fr = lr.predict(poly_var_test)\n",
    "\n",
    "print(f\"spearman correlation for the test dataset (unseen data) : {get_spearman_cor(y_test_predict_fr, y_test_fr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>my_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    my_col\n",
       "12      12\n",
       "13      13\n",
       "14      14\n",
       "15      15\n",
       "16      16\n",
       "17      17"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [i for i in range(20)]\n",
    "df_test = pd.DataFrame(data=data, columns=['my_col'])\n",
    "pd.concat([df_test[0:0], df_test[12:18]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYNOMIAL DEGREE 1\n",
      "FRANCE : \n",
      "[np.float64(0.17021310376295123), np.float64(0.022314226219918052), np.float64(0.09702425666444212), np.float64(0.07191577970334817), np.float64(0.09838362003163147)]\n",
      "0.09197019727645821\n",
      "\n",
      "GERMANY : \n",
      "[np.float64(0.26558027728085876), np.float64(0.2584920344259293), np.float64(0.4046567531305904), np.float64(0.32632530672038085), np.float64(0.24853533094812166)]\n",
      "0.3007179405011762\n",
      "\n",
      "\n",
      "POLYNOMIAL DEGREE 2\n",
      "FRANCE : \n",
      "[np.float64(0.055272147268799705), np.float64(0.062462979586104146), np.float64(0.06220045167764974), np.float64(-0.017793003218104435), np.float64(-0.13892807200737664)]\n",
      "0.06733133075160694\n",
      "\n",
      "GERMANY : \n",
      "[np.float64(0.19138528622540255), np.float64(-0.05747573704449734), np.float64(-0.09420281753130592), np.float64(-0.004617972898736495), np.float64(-0.08694096601073346)]\n",
      "0.08692455594213515\n",
      "\n",
      "\n",
      "POLYNOMIAL DEGREE 3\n",
      "FRANCE : \n",
      "[np.float64(-0.00014289151358794366), np.float64(0.006325071292920781), np.float64(-0.10418581655972732), np.float64(-0.12828084830942657), np.float64(0.06756065241418899)]\n",
      "0.06129905601797032\n",
      "\n",
      "GERMANY : \n",
      "[np.float64(-0.032379248658318434), np.float64(0.2958134956967588), np.float64(0.18864601967799646), np.float64(0.05086637062809009), np.float64(0.12999776386404294)]\n",
      "0.13954057970504136\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cv_for_spearman_correlation_polynomial(n, df_x_set, df_y_set, polynomial_degree=2):\n",
    "    x_set = df_x_set.copy()\n",
    "    y_set = df_y_set.copy()\n",
    "    x_set.reset_index(inplace=True)\n",
    "    y_set.reset_index(inplace=True)\n",
    "    rows = x_set.shape[0]\n",
    "    spearman_results = []\n",
    "    for i in range(n):\n",
    "        x_train_set = pd.concat([x_set[0:round(i*rows/n)], x_set[round(i*rows/5+rows/n):851]])\n",
    "        y_train_set = pd.concat([y_set[0:round(i*rows/n)], y_set[round(i*rows/5+rows/n):851]])\n",
    "        x_test_set = x_set[round(i*rows/n):round(i*rows/5+rows/n)]\n",
    "        y_test_set = y_set[round(i*rows/n):round(i*rows/5+rows/n)]\n",
    "    \n",
    "        poly_var_train = PolynomialFeatures(degree=polynomial_degree).fit_transform(x_train_set)\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(poly_var_train, y_train_set['TARGET'])\n",
    "        poly_var_test = PolynomialFeatures(degree=polynomial_degree).fit_transform(x_test_set)\n",
    "        y_predict = lr.predict(poly_var_test)\n",
    "        spearman_results.append(get_spearman_cor(y_predict, y_test_set))\n",
    "    return spearman_results\n",
    "\n",
    "for i in range(1, 3):\n",
    "    print(f'POLYNOMIAL DEGREE {i}')\n",
    "    print('FRANCE : ')\n",
    "    results = cv_for_spearman_correlation_polynomial(5, df_X_train_fr, df_y_train_fr, i)\n",
    "    print(results)\n",
    "    results = [abs(r) for r in results]\n",
    "    print(mean(results))\n",
    "    print()\n",
    "    print('GERMANY : ')\n",
    "    results = cv_for_spearman_correlation_polynomial(5, df_X_train_de, df_y_train_de, i)\n",
    "    print(results)\n",
    "    results = [abs(r) for r in results]\n",
    "    print(mean(results))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial regression with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization term 1\n",
      "POLYNOMIAL DEGREE 1\n",
      "FRANCE : \n",
      "[np.float64(0.17021310376295123), np.float64(0.031361557391564536), np.float64(0.10416661666724668), np.float64(0.07637838068893082), np.float64(0.0937549233333944)]\n",
      "0.09517491636881753\n",
      "\n",
      "GERMANY : \n",
      "[np.float64(0.28419051878354207), np.float64(0.260649377403406), np.float64(0.41642441860465124), np.float64(0.34788156930965025), np.float64(0.25188953488372096)]\n",
      "0.3122070837969941\n",
      "\n",
      "\n",
      "POLYNOMIAL DEGREE 2\n",
      "FRANCE : \n",
      "[np.float64(0.00891667470688527), np.float64(0.06392364482385916), np.float64(0.02511585935125997), np.float64(0.04517436996598702), np.float64(-0.09467089233700333)]\n",
      "0.047560288236998954\n",
      "\n",
      "GERMANY : \n",
      "[np.float64(0.03584525939177102), np.float64(0.20356848562534333), np.float64(0.040172182468694105), np.float64(-0.007536394433253982), np.float64(0.2769286672629696)]\n",
      "0.1128101978364064\n",
      "\n",
      "\n",
      "regularization term 10\n",
      "POLYNOMIAL DEGREE 1\n",
      "FRANCE : \n",
      "[np.float64(0.1740430848451033), np.float64(0.03520129945469313), np.float64(0.10275782455647842), np.float64(0.08976129847765953), np.float64(0.09462204065681086)]\n",
      "0.09927710959814905\n",
      "\n",
      "GERMANY : \n",
      "[np.float64(0.2982278622540251), np.float64(0.2660799304156748), np.float64(0.43502347942754926), np.float64(0.344179179637429), np.float64(0.25337097495527733)]\n",
      "0.3193762853379911\n",
      "\n",
      "\n",
      "POLYNOMIAL DEGREE 2\n",
      "FRANCE : \n",
      "[np.float64(0.0037628098578158494), np.float64(0.054302306409951095), np.float64(-0.0036311796654043315), np.float64(0.008821392150756285), np.float64(-0.08861084140912673)]\n",
      "0.031825705898610855\n",
      "\n",
      "GERMANY : \n",
      "[np.float64(0.11739713774597497), np.float64(0.2463262223036074), np.float64(0.13305567978533098), np.float64(0.10588147775132759), np.float64(0.23763416815742402)]\n",
      "0.168058937148733\n",
      "\n",
      "\n",
      "regularization term 50\n",
      "POLYNOMIAL DEGREE 1\n",
      "FRANCE : \n",
      "[np.float64(0.1849052824677629), np.float64(0.049360958958482186), np.float64(0.10783379613105223), np.float64(0.09844468463187206), np.float64(0.08752144894083451)]\n",
      "0.10561323422600077\n",
      "\n",
      "GERMANY : \n",
      "[np.float64(0.31535107334525947), np.float64(0.24941059329793078), np.float64(0.44947450805008954), np.float64(0.3374896996887017), np.float64(0.23823792486583187)]\n",
      "0.31799275984956266\n",
      "\n",
      "\n",
      "POLYNOMIAL DEGREE 2\n",
      "FRANCE : \n",
      "[np.float64(-0.009661664649523267), np.float64(0.06940968850947417), np.float64(0.01797109936189533), np.float64(-0.00030165912518853697), np.float64(-0.11521058127392969)]\n",
      "0.0425109385840022\n",
      "\n",
      "GERMANY : \n",
      "[np.float64(0.14050201252236139), np.float64(0.23514466214978938), np.float64(0.27843246869409666), np.float64(0.159219923091009), np.float64(0.20726185152057247)]\n",
      "0.20411218359556577\n",
      "\n",
      "\n",
      "regularization term 100\n",
      "POLYNOMIAL DEGREE 1\n",
      "FRANCE : \n",
      "[np.float64(0.19569664498112246), np.float64(0.06234085038562296), np.float64(0.10730339910127408), np.float64(0.10060881406439873), np.float64(0.08240423544067269)]\n",
      "0.10967078879461818\n",
      "\n",
      "GERMANY : \n",
      "[np.float64(0.32185822898032207), np.float64(0.23762818165171215), np.float64(0.458038908765653), np.float64(0.3329918970884453), np.float64(0.23303890876565297)]\n",
      "0.3167112250503571\n",
      "\n",
      "\n",
      "POLYNOMIAL DEGREE 2\n",
      "FRANCE : \n",
      "[np.float64(-0.015269851234103073), np.float64(0.07656890224168149), np.float64(0.0352366026751265), np.float64(-0.0023729703653495035), np.float64(-0.1265270729905167)]\n",
      "0.05119507990135545\n",
      "\n",
      "GERMANY : \n",
      "[np.float64(0.15833519677996424), np.float64(0.23057819080754438), np.float64(0.3259950805008945), np.float64(0.16550311298297013), np.float64(0.21229315742397142)]\n",
      "0.21854094769906893\n",
      "\n",
      "\n",
      "regularization term 1000\n",
      "POLYNOMIAL DEGREE 1\n",
      "FRANCE : \n",
      "[np.float64(0.17924091631373823), np.float64(0.11735028486636012), np.float64(0.07255879367096442), np.float64(0.07862311539377509), np.float64(0.03914607263023553)]\n",
      "0.09738383657501468\n",
      "\n",
      "GERMANY : \n",
      "[np.float64(0.3386739713774598), np.float64(0.19889328877494966), np.float64(0.43228980322003585), np.float64(0.3199448361106024), np.float64(0.21038126118067982)]\n",
      "0.3000366321327455\n",
      "\n",
      "\n",
      "POLYNOMIAL DEGREE 2\n",
      "FRANCE : \n",
      "[np.float64(-0.0220553661870485), np.float64(0.09580669390147839), np.float64(0.08197154095960671), np.float64(0.025027937054610074), np.float64(-0.13919675624843525)]\n",
      "0.07281165887023579\n",
      "\n",
      "GERMANY : \n",
      "[np.float64(0.2217016994633274), np.float64(0.2102407983885735), np.float64(0.35121869409660117), np.float64(0.22593732832814498), np.float64(0.27540809481216466)]\n",
      "0.25690132301776236\n",
      "\n",
      "\n",
      "regularization term 10000\n",
      "POLYNOMIAL DEGREE 1\n",
      "FRANCE : \n",
      "[np.float64(0.11610485061764991), np.float64(0.06514005166065182), np.float64(0.05604928612415913), np.float64(0.02951984904830821), np.float64(-0.02388725032211577)]\n",
      "0.05814025755457697\n",
      "\n",
      "GERMANY : \n",
      "[np.float64(0.15936940966010735), np.float64(0.12251648049807727), np.float64(0.3796288014311271), np.float64(0.31344419520234384), np.float64(0.1163405635062612)]\n",
      "0.21825989005958335\n",
      "\n",
      "\n",
      "POLYNOMIAL DEGREE 2\n",
      "FRANCE : \n",
      "[np.float64(-0.034463723264428904), np.float64(0.10570160172446433), np.float64(0.11252096988289272), np.float64(0.04202343659357234), np.float64(-0.13219631047685348)]\n",
      "0.08538120838844235\n",
      "\n",
      "GERMANY : \n",
      "[np.float64(0.2596153846153847), np.float64(0.21385162973814317), np.float64(0.284827817531306), np.float64(0.2648610602453763), np.float64(0.3698457066189625)]\n",
      "0.2786003197498345\n",
      "\n",
      "\n",
      "regularization term 100000.0\n",
      "POLYNOMIAL DEGREE 1\n",
      "FRANCE : \n",
      "[np.float64(0.007451120721367899), np.float64(0.004151171524355617), np.float64(0.030667028264730035), np.float64(-0.0445759368836292), np.float64(-0.07038427952931406)]\n",
      "0.03144590738467936\n",
      "\n",
      "GERMANY : \n",
      "[np.float64(-0.11761516100178894), np.float64(-0.05364173228346457), np.float64(0.09797070661896244), np.float64(0.12869094488188978), np.float64(-0.11848166368515207)]\n",
      "0.10328004169425156\n",
      "\n",
      "\n",
      "POLYNOMIAL DEGREE 2\n",
      "FRANCE : \n",
      "[np.float64(-0.02930741582538362), np.float64(0.09518872014704358), np.float64(0.14355039611819517), np.float64(0.05736286417400969), np.float64(-0.12470734790334696)]\n",
      "0.0900233488335958\n",
      "\n",
      "GERMANY : \n",
      "[np.float64(0.27199239713774603), np.float64(0.22619483611060245), np.float64(0.26236583184257606), np.float64(0.28754463468229263), np.float64(0.39222383720930243)]\n",
      "0.2880643073965039\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cv_for_spearman_correlation_poly_with_regu(n, df_x_set, df_y_set, polynomial_degree=2, regularization=10):\n",
    "    x_set = df_x_set.copy()\n",
    "    y_set = df_y_set.copy()\n",
    "    x_set.reset_index(inplace=True)\n",
    "    y_set.reset_index(inplace=True)\n",
    "    rows = x_set.shape[0]\n",
    "    spearman_results = []\n",
    "    for i in range(n):\n",
    "        x_train_set = pd.concat([x_set[0:round(i*rows/n)], x_set[round(i*rows/5+rows/n):851]])\n",
    "        y_train_set = pd.concat([y_set[0:round(i*rows/n)], y_set[round(i*rows/5+rows/n):851]])\n",
    "        x_test_set = x_set[round(i*rows/n):round(i*rows/5+rows/n)]\n",
    "        y_test_set = y_set[round(i*rows/n):round(i*rows/5+rows/n)]\n",
    "    \n",
    "        poly_var_train = PolynomialFeatures(degree=polynomial_degree).fit_transform(x_train_set)\n",
    "        model = Ridge(alpha=regularization)\n",
    "        model.fit(poly_var_train, y_train_set['TARGET'])\n",
    "        poly_var_test = PolynomialFeatures(degree=polynomial_degree).fit_transform(x_test_set)\n",
    "        y_predict = model.predict(poly_var_test)\n",
    "        spearman_results.append(get_spearman_cor(y_predict, y_test_set))\n",
    "    return spearman_results\n",
    "for u in [1, 10, 50, 100, 1000, 10000, 1e5]:\n",
    "    print(f'regularization term {u}')\n",
    "    for i in range(1, 3):\n",
    "        print(f'POLYNOMIAL DEGREE {i}')\n",
    "        print('FRANCE : ')\n",
    "        results = cv_for_spearman_correlation_poly_with_regu(5, df_X_train_fr, df_y_train_fr, i, u)\n",
    "        print(results)\n",
    "        results = [abs(r) for r in results]\n",
    "        print(mean(results))\n",
    "        print()\n",
    "        print('GERMANY : ')\n",
    "        results = cv_for_spearman_correlation_poly_with_regu(5, df_X_train_de, df_y_train_de, i, u)\n",
    "        print(results)\n",
    "        results = [abs(r) for r in results]\n",
    "        print(mean(results))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression with different loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial regression with cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging, Random Forest, Extra Trees, Boosting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
